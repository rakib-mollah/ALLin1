# ğŸ”‘ Key GitHub Repositories for LLM Fine-Tuning, Quantization, LoRA, and Distillation

A curated map of open-source tools to customize and optimize large language models.  
Youâ€™ll find starter tutorials and production-grade libraries across four buckets: **Fine-Tuning**, **Quantization**, **LoRA**, and **Distillation**.  

---

## ğŸ“‘ Table of Contents
- [ğŸ¯ Fine-Tuning LLMs](#-fine-tuning-llms)
- [âš¡ Quantization](#-quantization)
- [ğŸª¶ LoRA (Low-Rank Adaptation)](#-lora-low-rank-adaptation)
- [ğŸ”„ Distillation](#-distillation)
- [ğŸ“š Further Reading](#-further-reading)

---

## ğŸ¯ Fine-Tuning LLMs

- ğŸ¤– [poloclub/Fine-tuning-LLMs](https://github.com/poloclub/Fine-tuning-LLMs) â€” Colab-friendly tutorial for fine-tuning open-source LLMs with QLoRA.  
- ğŸ§° [georgian-io/LLM-Finetuning-Toolkit](https://github.com/georgian-io/LLM-Finetuning-Toolkit) â€” Config-driven CLI with YAML pipelines for experiment tracking.  
- ğŸ“˜ [dvgodoy/FineTuningLLMs](https://github.com/dvgodoy/FineTuningLLMs) â€” Companion code to a practical guidebook; includes quantization, LoRA, and dataset prep.  
- ğŸ—ï¸ [hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) â€” End-to-end fine-tuning and serving for LLaMA-family models.  

---

## âš¡ Quantization

- ğŸ”§ [vllm-project/llm-compressor](https://github.com/vllm-project/llm-compressor) â€” Implements GPTQ and SmoothQuant; integrates with vLLM.  
- ğŸ§® [facebookresearch/LLM-QAT](https://github.com/facebookresearch/LLM-QAT) â€” Data-free Quantization-Aware Training (down to 4-bit).  
- ğŸ“ [ruikangliu/FlatQuant](https://github.com/ruikangliu/FlatQuant) â€” Fake-quantization for W4A4 inference; per-layer optimization.  
- ğŸŒ [pprp/Awesome-LLM-Quantization](https://github.com/pprp/Awesome-LLM-Quantization) â€” Curated list of PTQ and QAT methods.  
- ğŸ“Š [Efficient-ML/Awesome-Model-Quantization](https://github.com/Efficient-ML/Awesome-Model-Quantization) â€” Broader quantization resources across models.  
- ğŸ›¡ï¸ [eth-sri/llm-quantization-attack](https://github.com/eth-sri/llm-quantization-attack) â€” Security research on attacking quantized LLMs.  
- ğŸ“ [Blog: GPTQ Primer](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html) â€” Practical intro with examples.  

---

## ğŸª¶ LoRA (Low-Rank Adaptation)

- ğŸ›ï¸ [microsoft/LoRA](https://github.com/microsoft/LoRA) â€” Original LoRA implementation; foundation for HF PEFT.  
- ğŸ“ [josephtkim/LoRA-fine-tune-RoBERTa](https://github.com/josephtkim/LoRA-fine-tune-RoBERTa) â€” LoRA applied to RoBERTa for Yelp polarity.  
- ğŸ§ª [tsmatz/finetune_llm_with_lora](https://github.com/tsmatz/finetune_llm_with_lora) â€” Hands-on LoRA walkthroughs across models.  
- ğŸ’¹ [Open-Finance-Lab/FinLoRA](https://github.com/Open-Finance-Lab/FinLoRA) â€” LoRA benchmarks on financial tasks.  
- âš¡ [fshnkarimi/Fine-tuning-an-LLM-using-LoRA](https://github.com/fshnkarimi/Fine-tuning-an-LLM-using-LoRA) â€” Minimal LoRA example for quick runs.  

---

## ğŸ”„ Distillation

- ğŸ§‘â€ğŸ« [arcee-ai/DistillKit](https://github.com/arcee-ai/DistillKit) â€” Teacher-student pipelines with logits and hidden states.  
- ğŸŒŠ [horus-ai-labs/DistillFlow](https://github.com/horus-ai-labs/DistillFlow/) â€” Multi-strategy distillation + fine-tuning support.  
- ğŸ“’ [predibase/llm_distillation_playbook](https://github.com/predibase/llm_distillation_playbook) â€” Production best practices for distillation.  
- ğŸ“° [jongwooko/distillm](https://github.com/jongwooko/distillm) â€” ICML DistiLLM implementation with LoRA checkpoints.  
- ğŸ”¬ [haitongli/knowledge-distillation-pytorch](https://github.com/haitongli/knowledge-distillation-pytorch) â€” KD templates in PyTorch.  
- ğŸ› ï¸ [modelscope/easydistill](https://github.com/modelscope/easydistill) â€” Simple KD recipes for quick use.  
- ğŸ“š [SforAiDl/KD_Lib](https://github.com/SforAiDl/KD_Lib) â€” KD loss functions and strategies.  
- ğŸŒ [Tebmer/Awesome-Knowledge-Distillation-of-LLMs](https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs) â€” Index of KD resources for LLMs.  
- ğŸ¯ [songmzhang/DSKD](https://github.com/songmzhang/DSKD) â€” Dataset-aware KD methods.  
- ğŸ” [tsitsimis/neural-network-distillation](https://github.com/tsitsimis/neural-network-distillation) â€” Classic KD patterns.  
- ğŸ§© [Guang000/Awesome-Dataset-Distillation](https://github.com/Guang000/Awesome-Dataset-Distillation) â€” Dataset distillation methods for small-data regimes.  
- ğŸ¤– [Nardien/agent-distillation](https://github.com/Nardien/agent-distillation) â€” Distilling agent-like behaviors.  
- ğŸ“ [jdeschena/sdtt](https://github.com/jdeschena/sdtt) â€” Seq2Seq distillation tooling.  
- ğŸ—‚ï¸ [Nicolas-BZRD/llm-recipes](https://github.com/Nicolas-BZRD/llm-recipes) â€” Mix of fine-tuning, KD, and eval recipes.  

---

## ğŸ“š Further Reading

- ğŸ“° [GitHub Blog](https://github.blog/ai-and-ml/llms/customizing-and-fine-tuning-llms-what-you-need-to-know/) â€” Customizing and fine-tuning LLMs: what you need to know.  
- ğŸ“– [Jack Young](https://jackyoung96.github.io/2024/08/08/llm-distillation-en/) â€” Clear introduction to LLM distillation (2024).  
