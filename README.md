# üîë Key GitHub Repositories for LLM Fine-Tuning, Quantization, LoRA, and Distillation

A quick, curated map of open-source tools you can use to customize and optimize large language models. You‚Äôll find starter tutorials and production-grade libraries across four buckets: **Fine-Tuning**, **Quantization**, **LoRA (parameter-efficient adaptation)**, and **Distillation**. Each entry links straight to the repo or article so you can dive in fast.

---

## üìë Table of Contents
- [üéØ Fine-Tuning LLMs](#fine-tuning-llms)
- [‚ö° Quantization](#quantization)
- [ü™∂ LoRA (Low-Rank Adaptation)](#lora-low-rank-adaptation)
- [üîÑ Distillation](#distillation)
- [üìö Further Reading](#further-reading)

---

## üéØ Fine-Tuning LLMs

- **poloclub/Fine-tuning-LLMs** ‚Äî Colab-friendly tutorial for fine-tuning open-source LLMs (e.g., Llama-2) with QLoRA.  
  https://github.com/poloclub/Fine-tuning-LLMs

- **georgian-io/LLM-Finetuning-Toolkit** ‚Äî Config-driven CLI that launches and tracks experiments via YAML pipelines.  
  https://github.com/georgian-io/LLM-Finetuning-Toolkit

- **dvgodoy/FineTuningLLMs** ‚Äî Companion code for a practical guidebook; includes quantization, LoRA, dataset prep, training, and querying.  
  https://github.com/dvgodoy/FineTuningLLMs

- **hiyouga/LLaMA-Factory** ‚Äî End-to-end fine-tuning and serving for LLaMA-family models with a clean UX.  
  https://github.com/hiyouga/LLaMA-Factory

---

## ‚ö° Quantization

- **vllm-project/llm-compressor** ‚Äî Algorithms like GPTQ and SmoothQuant for int8 weight/activation quantization; integrates with vLLM.  
  https://github.com/vllm-project/llm-compressor

- **facebookresearch/LLM-QAT** ‚Äî Data-free Quantization-Aware Training; supports weight, activation, and KV-cache quantization down to 4-bit.  
  https://github.com/facebookresearch/LLM-QAT

- **ruikangliu/FlatQuant** ‚Äî Fake-quantization for W4A4 inference with vLLM; per-layer affine transform optimization.  
  https://github.com/ruikangliu/FlatQuant

- **pprp/Awesome-LLM-Quantization** ‚Äî Curated list of PTQ and QAT methods for LLMs.  
  https://github.com/pprp/Awesome-LLM-Quantization

- **Efficient-ML/Awesome-Model-Quantization** ‚Äî Broader quantization landscape across models and tasks.  
  https://github.com/Efficient-ML/Awesome-Model-Quantization

- **eth-sri/llm-quantization-attack** ‚Äî Security angle: analyses and assets on attacking quantized LLMs.  
  https://github.com/eth-sri/llm-quantization-attack

- **üìù Blog** ‚Äî Practical GPTQ primer with working examples.  
  https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html

---

## ü™∂ LoRA (Low-Rank Adaptation)

- **microsoft/LoRA** ‚Äî The original LoRA implementation; now influences/feeds into HF PEFT.  
  https://github.com/microsoft/LoRA

- **josephtkim/LoRA-fine-tune-RoBERTa** ‚Äî Notebook showing LoRA applied to RoBERTa for Yelp polarity with strong parameter savings.  
  https://github.com/josephtkim/LoRA-fine-tune-RoBERTa

- **tsmatz/finetune_llm_with_lora** ‚Äî Listed above; hands-on LoRA walkthroughs for multiple model families.  
  https://github.com/tsmatz/finetune_llm_with_lora

- **Open-Finance-Lab/FinLoRA** ‚Äî Benchmarking LoRA variants on financial tasks; tracks accuracy/F1/BERTScore and cost.  
  https://github.com/Open-Finance-Lab/FinLoRA

- **fshnkarimi/Fine-tuning-an-LLM-using-LoRA** ‚Äî Minimal example wired for quick experiments.  
  https://github.com/fshnkarimi/Fine-tuning-an-LLM-using-LoRA

---

## üîÑ Distillation

- **arcee-ai/DistillKit** ‚Äî Teacher-student workflows using logits and hidden states; focused on practical pipelines.  
  https://github.com/arcee-ai/DistillKit

- **horus-ai-labs/DistillFlow** ‚Äî Scalable, multi-strategy distillation (logits, attention, layer-wise) plus fine-tuning and resource allocation.  
  https://github.com/horus-ai-labs/DistillFlow/

- **predibase/llm_distillation_playbook** ‚Äî Industry-leaning playbook of best practices for LLM distillation in production.  
  https://github.com/predibase/llm_distillation_playbook

- **jongwooko/distillm** ‚Äî ICML-published DistiLLM implementation with LoRA checkpoint release.  
  https://github.com/jongwooko/distillm

- **haitongli/knowledge-distillation-pytorch** ‚Äî General KD templates in PyTorch you can adapt for LLMs.  
  https://github.com/haitongli/knowledge-distillation-pytorch

- **modelscope/easydistill** ‚Äî Simple KD recipes and utilities for quick iteration.  
  https://github.com/modelscope/easydistill

- **SforAiDl/KD_Lib** ‚Äî Library of KD losses and strategies.  
  https://github.com/SforAiDl/KD_Lib

- **Tebmer/Awesome-Knowledge-Distillation-of-LLMs** ‚Äî Big-picture index of KD resources specific to LLMs.  
  https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs

- **songmzhang/DSKD** ‚Äî Dataset-aware KD methods.  
  https://github.com/songmzhang/DSKD

- **tsitsimis/neural-network-distillation** ‚Äî Classic KD patterns and examples.  
  https://github.com/tsitsimis/neural-network-distillation

- **Guang000/Awesome-Dataset-Distillation** ‚Äî Dataset distillation techniques that pair well with KD for small-data regimes.  
  https://github.com/Guang000/Awesome-Dataset-Distillation

- **Nardien/agent-distillation** ‚Äî Early work on distilling agentic behaviors.  
  https://github.com/Nardien/agent-distillation

- **jdeschena/sdtt** ‚Äî Sequence-to-sequence distillation tooling.  
  https://github.com/jdeschena/sdtt

- **Nicolas-BZRD/llm-recipes** ‚Äî Mixed bag of fine-tuning, KD, and eval recipes worth browsing.  
  https://github.com/Nicolas-BZRD/llm-recipes


---

## üìö Further Reading

- **GitHub Blog** ‚Äî Customizing and fine-tuning LLMs: what you need to know.  
  https://github.blog/ai-and-ml/llms/customizing-and-fine-tuning-llms-what-you-need-to-know/

- **Jack Young** ‚Äî A readable intro to LLM distillation (2024).  
  https://jackyoung96.github.io/2024/08/08/llm-distillation-en/
